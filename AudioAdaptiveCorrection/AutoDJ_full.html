<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Phantom DJ - Hive Mind Edition</title>

    <script src="https://unpkg.com/meyda/dist/web/meyda.min.js"></script>

    <style>
        :root {
            --bg-color: #121212;
            --panel-color: #1e1e1e;
            --accent-color: #00ffcc;
            --dislike-color: #ff3366;
            --like-color: #33ff99;
            --text-color: #e0e0e0;
            --warn-color: #ffaa00;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            height: 100vh;
            overflow: hidden;
        }

        /* UI COMPONENTS */
        header {
            width: 100%; padding: 1rem; background-color: var(--panel-color);
            display: flex; justify-content: space-between; align-items: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.5); z-index: 10;
        }
        h1 { margin: 0; font-size: 1.5rem; color: var(--accent-color); text-transform: uppercase; letter-spacing: 2px; }
        .controls { display: flex; gap: 10px; align-items: center; }

        button, select {
            background-color: #333; color: white; border: 1px solid #555;
            padding: 8px 16px; border-radius: 4px; cursor: pointer;
            transition: all 0.2s; font-weight: bold; font-family: inherit;
        }
        button:hover, select:hover { background-color: #444; border-color: var(--accent-color); }
        button:disabled { opacity: 0.5; cursor: not-allowed; }
        button.active { background-color: var(--accent-color); color: #000; border-color: var(--accent-color); }

        #fileInput { display: none; }
        .file-label {
            background-color: var(--accent-color); color: #000; padding: 8px 16px;
            border-radius: 4px; cursor: pointer; font-weight: bold;
        }

        /* MAIN CANVAS AREA */
        #main-container {
            position: relative; width: 100%; flex-grow: 1;
            display: flex; justify-content: center; align-items: center;
        }
        canvas {
            background: radial-gradient(circle at center, #2a2a2a 0%, #000000 100%);
            box-shadow: inset 0 0 50px #000; cursor: crosshair;
        }

        /* OVERLAYS & PANELS */
        #voting-panel {
            position: absolute; bottom: 80px; left: 50%; transform: translateX(-50%);
            display: flex; gap: 20px; pointer-events: none; opacity: 0; transition: opacity 0.3s;
        }
        #voting-panel.visible { opacity: 1; pointer-events: auto; }
        .vote-btn {
            width: 60px; height: 60px; border-radius: 50%; border: 2px solid #fff;
            font-size: 24px; display: flex; align-items: center; justify-content: center;
            cursor: pointer; background: rgba(0,0,0,0.6); transition: transform 0.2s;
        }
        .vote-btn:hover { transform: scale(1.1); }
        .vote-like:hover { background-color: var(--like-color); border-color: var(--like-color); color: #000; }
        .vote-dislike:hover { background-color: var(--dislike-color); border-color: var(--dislike-color); color: white; }

        #dev-panel {
            position: absolute; top: 80px; right: 20px; width: 300px;
            background-color: rgba(0, 0, 0, 0.95); border: 1px solid var(--accent-color);
            border-radius: 8px; padding: 15px; z-index: 100; display: none;
            box-shadow: 0 4px 15px rgba(0,0,0,0.8); font-size: 0.85rem;
        }
        #dev-panel.visible { display: block; }
        .kernel-viz { display: flex; gap: 2px; margin-bottom: 15px; height: 30px; }
        .kernel-cell { flex: 1; height: 100%; border: 1px solid #000; display: flex; align-items: center; justify-content: center; font-size: 0.6rem; color: #fff; }
        .param-row { display: flex; justify-content: space-between; margin-bottom: 5px; border-bottom: 1px solid #333; }

        #status {
            position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%);
            background-color: rgba(0,0,0,0.7); padding: 10px 20px; border-radius: 20px;
            pointer-events: none; border: 1px solid #444; transition: opacity 0.3s;
        }

        #harmonic-info {
            position: absolute; top: 20px; left: 20px; background-color: rgba(0,0,0,0.6);
            padding: 10px; border-radius: 8px; pointer-events: none; font-family: monospace; color: var(--accent-color);
        }

        #sync-status {
            position: fixed; top: 10px; right: 10px; width: 12px; height: 12px; border-radius: 50%;
            background-color: #555; z-index: 999; box-shadow: 0 0 8px rgba(0,0,0,0.5);
        }
        #sync-status.online { background-color: var(--like-color); box-shadow: 0 0 8px var(--like-color); }
        #sync-status.syncing { background-color: var(--warn-color); animation: blink 1s infinite; }
        #sync-status.offline { background-color: var(--dislike-color); }
        @keyframes blink { 50% { opacity: 0.5; } }

        #overlay {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(0,0,0,0.9); display: flex; flex-direction: column;
            justify-content: center; align-items: center; z-index: 20; text-align: center;
        }
    </style>
</head>
<body>

    <div id="sync-status" title="Connection Status"></div>

    <header>
        <h1>Phantom DJ <span style="font-size:0.8rem; color:#888;">// Hive Mind</span></h1>
        <div class="controls">
            <select id="beatSelect">
                <option value="4.0" selected>4 Beats</option>
                <option value="8.0">8 Beats</option>
                <option value="16.0">16 Beats</option>
            </select>
            <button id="btn-dev">üß† Neural View</button>
            <button id="btn-play" disabled>‚ñ∂ PLAY</button>
            <button id="btn-stop" disabled>‚ñ† STOP</button>
            <label for="fileInput" class="file-label">+ Load Tracks</label>
            <input type="file" id="fileInput" accept="audio/*" multiple>
        </div>
    </header>

    <div id="main-container">
        <canvas id="djCanvas"></canvas>
        <div id="harmonic-info">Waiting for audio...</div>

        <div id="voting-panel">
            <button class="vote-btn vote-dislike" onclick="voteTransition(false)">üëé</button>
            <button class="vote-btn vote-like" onclick="voteTransition(true)">üëç</button>
        </div>

        <div id="dev-panel">
            <h3>Shared Neural Layer</h3>
            <p style="color:#888; font-size:0.7rem;">CNN Kernel (Timbre)</p>
            <div class="kernel-viz" id="cnn-viz"></div>
            <p style="color:#888; font-size:0.7rem; margin-top:10px;">Decision Weights</p>
            <div class="param-row"><span>Vs. Current</span><span id="val-current">--</span></div>
            <div class="param-row"><span>Vs. Next Natural</span><span id="val-nextnat">--</span></div>
            <div class="param-row"><span>Harmonic</span><span id="val-harmony">--</span></div>
            <div class="param-row"><span>BPM</span><span id="val-bpm">--</span></div>
            <div style="margin-top:10px; font-size:0.7rem; color:#aaa; text-align:center;">
                Status: <span id="db-status-text">Connecting...</span>
            </div>
        </div>

        <div id="status">Connecting to Hive Mind...</div>

        <div id="overlay">
            <h2 style="color:var(--accent-color)">Phantom DJ: Hive Mind</h2>
            <p style="max-width:600px; line-height:1.6; color:#ccc;">
                1. Load audio files.<br>
                2. The AI is <strong>Shared</strong>: Your votes update the global brain.<br>
                3. Votes are logged to the cloud dataset.<br>
                4. Click nodes on the map to preview how the AI would mix its way there.
            </p>
            <label for="fileInput" class="file-label" style="font-size: 1.2rem; padding: 15px 30px;">LOAD AUDIO</label>
        </div>
    </div>

    <script type="module">
        import { initializeApp } from "https://www.gstatic.com/firebasejs/10.7.1/firebase-app.js";
        import { getFirestore, doc, getDoc, setDoc, addDoc, collection, runTransaction } from "https://www.gstatic.com/firebasejs/10.7.1/firebase-firestore.js";
        import { getAuth, signInAnonymously } from "https://www.gstatic.com/firebasejs/10.7.1/firebase-auth.js";

        // ==========================================
        // ‚ö†Ô∏è INSERT YOUR FIREBASE CONFIG HERE ‚ö†Ô∏è
        // ==========================================
        const firebaseConfig = {
          apiKey: "AIzaSyDJIzSP6ci_3djwGgwqnBlBGbcqTQLOcRQ",
          authDomain: "autodj-b79f7.firebaseapp.com",
          projectId: "autodj-b79f7",
          storageBucket: "autodj-b79f7.firebasestorage.app",
          messagingSenderId: "125149780002",
          appId: "1:125149780002:web:951a3acb1c6e648dd2127b",
          measurementId: "G-Y5648S9L7Q"
        };

        // Initialize Firebase
        const app = initializeApp(firebaseConfig);
        const db = getFirestore(app);
        const auth = getAuth(app);

        // Expose Firebase to Global Scope for the App Logic
        window.db = db;
        window.auth = auth;
        window.doc = doc;
        window.getDoc = getDoc;
        window.setDoc = setDoc;
        window.addDoc = addDoc;
        window.collection = collection;
        window.runTransaction = runTransaction;
        window.signInAnonymously = signInAnonymously;

        console.log("Firebase Modules Loaded.");
    </script>

    <script>
        /**
         * PHANTOM DJ - HIVE MIND EDITION
         * Logic: Real-time Federated Averaging via Firestore Transactions
         * This version:
         *  - Removes pitch data augmentation
         *  - Adds click-to-select segment + path prediction to that segment
         */

        // --- CONSTANTS & CONFIG ---
        const BRAIN_COLLECTION = "phantom_brains";
        const BRAIN_DOC_ID = "global_v1";
        const DATASET_COLLECTION = "training_dataset";

        const CONFIG = {
            defaultBpm: 120,
            beatsPerSegment: 4,
            crossfade: 0.05,
            maxSamples: 1500,
            silenceThreshold: 0.01,
            bufferSize: 4096,
            mixThreshold: 8.0,
            learningRate: 0.05,
            momentum: 0.9,
            batchSize: 16,
            memorySize: 200,

            // Precomputed neighbours & exploration
            maxNeighbors: 64,
            selectionTemperature: 0.8
        };

        // Fade Curves
        const FADE_RES = 100;
        const FADE_IN_CURVE = new Float32Array(FADE_RES);
        const FADE_OUT_CURVE = new Float32Array(FADE_RES);
        for (let i = 0; i < FADE_RES; i++) {
            const x = i / (FADE_RES - 1);
            FADE_IN_CURVE[i] = Math.sin(x * Math.PI / 2);
            FADE_OUT_CURVE[i] = Math.cos(x * Math.PI / 2);
        }

        // --- NEURAL MODEL STATE ---
        let AI_MODEL = {
            cnnKernel: new Array(13).fill(0.5),

            // pairwise distances
            contextWeights: {
                wCurrent: 2.0,
                wNextNat: 4.0,
                wPrev1: 1.0,
                wPrev2: 0.5
            },

            // global dense features
            denseWeights: {
                posFlow: 3.0,       // backwards vs forwards in track
                harmony: 2.0,       // circle-of-fifths distance
                bpm: 3.5,           // tempo mismatch
                rmsDelta: 1.0,      // loudness jump
                flatnessDelta: 1.0, // brightness jump
                relPosJump: 1.2,    // timeline jump
                trackRepeat: 1.5    // repeating same track too soon
            }
        };

        let MODEL_MOMENTUM = {
            cnnKernel: new Array(13).fill(0),
            contextWeights: { wCurrent: 0, wNextNat: 0, wPrev1: 0, wPrev2: 0 },
            denseWeights: {
                posFlow: 0,
                harmony: 0,
                bpm: 0,
                rmsDelta: 0,
                flatnessDelta: 0,
                relPosJump: 0,
                trackRepeat: 0
            }
        };

        const NOTES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];

        // --- APP STATE ---
        let audioCtx, loadedTracks = [], sampleLibrary = [], trailHistory = [], playQueue = [];
        let isPlaying = false, nextStartTime = 0, currentSampleIndex = -1, prevSampleIndex = -1, userNextSelection = -1;
        let playLoopId, canvas, ctx, currentBpm = 120, lastEffectiveKeyIndex = -1;
        let contextHistory = [], lastInputVector = null, transitionGraph = [];
        let previewPath = [], targetSampleIndex = -1;

        // --- CLOUD SYNC LOGIC ---

        async function initBrain() {
            updateStatus("Connecting to Hive Mind...");
            document.getElementById('sync-status').className = 'syncing';

            // Wait briefly for Firebase module to be attached to window
            if (!window.auth) { setTimeout(initBrain, 500); return; }

            try {
                await window.signInAnonymously(window.auth);
                const docRef = window.doc(window.db, BRAIN_COLLECTION, BRAIN_DOC_ID);
                const docSnap = await window.getDoc(docRef);

                if (docSnap.exists()) {
                    const data = docSnap.data();
                    if (data.cnnKernel) AI_MODEL.cnnKernel = data.cnnKernel;
                    if (data.contextWeights) Object.assign(AI_MODEL.contextWeights, data.contextWeights);
                    if (data.denseWeights) Object.assign(AI_MODEL.denseWeights, data.denseWeights);

                    document.getElementById('db-status-text').innerText = "Connected: Global Brain";
                    document.getElementById('db-status-text').style.color = "#00ffcc";
                    console.log("Global Brain Loaded.");
                } else {
                    console.log("Brain not found, creating genesis...");
                    await window.setDoc(docRef, AI_MODEL);
                    document.getElementById('db-status-text').innerText = "Genesis Created";
                }
                document.getElementById('sync-status').className = 'online';
            } catch (e) {
                console.error("Firebase Init Error:", e);
                document.getElementById('db-status-text').innerText = "Offline Mode";
                document.getElementById('db-status-text').style.color = "#ff3366";
                document.getElementById('sync-status').className = 'offline';
            }
            updateNeuralUI();
            updateStatus("Ready. Click nodes to preview paths.");
        }

        async function logTrainingData(vector, label) {
            if (!window.db) return;
            try {
                await window.addDoc(window.collection(window.db, DATASET_COLLECTION), {
                    timestamp: new Date(),
                    features: vector,
                    vote: label,
                    uid: window.auth.currentUser ? window.auth.currentUser.uid : 'anon'
                });
                console.log("Dataset: Logged vote to cloud.");
            } catch (e) { console.error("Dataset Log Error:", e); }
        }

        async function updateGlobalBrainTransaction() {
            if (!window.db) return;
            updateStatus("Syncing Mind...");
            document.getElementById('sync-status').className = 'syncing';
            const docRef = window.doc(window.db, BRAIN_COLLECTION, BRAIN_DOC_ID);

            try {
                await window.runTransaction(window.db, async (transaction) => {
                    const sfDoc = await transaction.get(docRef);
                    if (!sfDoc.exists()) { transaction.set(docRef, AI_MODEL); return; }

                    const globalBrain = sfDoc.data();

                    // MERGE STRATEGY: 90% Global Stability, 10% Local Innovation
                    const learningRate = 0.1;
                    const newBrain = JSON.parse(JSON.stringify(globalBrain));

                    const mergeArr = (g, l) => g.map((val, i) =>
                        (val * (1 - learningRate)) + ((l[i] ?? 0) * learningRate)
                    );

                    const mergeObj = (g, l) => {
                        const res = {};
                        const keys = new Set([
                            ...Object.keys(g || {}),
                            ...Object.keys(l || {})
                        ]);
                        keys.forEach(k => {
                            const gv = (g && typeof g[k] === "number") ? g[k] : 0;
                            const lv = (l && typeof l[k] === "number") ? l[k] : 0;
                            res[k] = gv * (1 - learningRate) + lv * learningRate;
                        });
                        return res;
                    };

                    newBrain.cnnKernel = mergeArr(globalBrain.cnnKernel || [], AI_MODEL.cnnKernel);
                    newBrain.contextWeights = mergeObj(globalBrain.contextWeights || {}, AI_MODEL.contextWeights);
                    newBrain.denseWeights = mergeObj(globalBrain.denseWeights || {}, AI_MODEL.denseWeights);

                    transaction.update(docRef, newBrain);
                });

                document.getElementById('sync-status').className = 'online';
                updateStatus("Hive Mind Updated.");
            } catch (e) {
                console.error("Sync Transaction Failed:", e);
                document.getElementById('sync-status').className = 'offline';
            }
        }

        // --- INTERACTION LOGIC ---

        async function voteTransition(isPositive) {
            if (!lastInputVector) return;

            // -1 is "good" (we want to reduce this cost), 1 is "bad"
            const label = isPositive ? -1 : 1;

            // 1. Add to Local Memory & Instant Train
            replayMemory.push({ input: lastInputVector, label: label });
            if (replayMemory.length > CONFIG.memorySize) replayMemory.shift();

            trainFromBuffer(); // Immediate local feedback

            // 2. Cloud Actions
            updateStatus("Uploading Knowledge...");
            await logTrainingData(lastInputVector, label);
            await updateGlobalBrainTransaction();

            document.getElementById('voting-panel').classList.remove('visible');
            lastInputVector = null;
        }

        let replayMemory = [];

        function trainFromBuffer() {
            if (replayMemory.length === 0) return;
            const batchSize = Math.min(replayMemory.length, CONFIG.batchSize);
            const batch = [];

            // Simple Random Sampling
            for (let i = 0; i < batchSize; i++) {
                batch.push(replayMemory[Math.floor(Math.random() * replayMemory.length)]);
            }

            // Gradient Accumulation
            let gradCNN = new Array(13).fill(0);
            let gradCtx = { wCurrent: 0, wNextNat: 0, wPrev1: 0, wPrev2: 0 };
            let gradDense = {
                posFlow: 0,
                harmony: 0,
                bpm: 0,
                rmsDelta: 0,
                flatnessDelta: 0,
                relPosJump: 0,
                trackRepeat: 0
            };

            batch.forEach(exp => {
                const dir = exp.label; // -1 good, +1 bad
                const vec = exp.input;

                for (let i = 0; i < 13; i++) {
                    gradCNN[i] += dir * vec.mfccDiff[i];
                }

                if (vec.distCurrent !== undefined) gradCtx.wCurrent += dir * vec.distCurrent;
                if (vec.distNextNat !== undefined) gradCtx.wNextNat += dir * vec.distNextNat;
                if (vec.distPrev1 !== undefined) gradCtx.wPrev1 += dir * vec.distPrev1;
                if (vec.distPrev2 !== undefined) gradCtx.wPrev2 += dir * vec.distPrev2;

                gradDense.posFlow       += dir * vec.posFlow;
                gradDense.harmony       += dir * (vec.harmony / 6.0);
                gradDense.bpm           += dir * vec.bpm;
                gradDense.rmsDelta      += dir * vec.rmsDelta;
                gradDense.flatnessDelta += dir * vec.flatnessDelta;
                gradDense.relPosJump    += dir * vec.relPosJump;
                gradDense.trackRepeat   += dir * vec.trackRepeat;
            });

            // Normalize
            for (let i = 0; i < 13; i++) gradCNN[i] /= batchSize;
            for (let k in gradCtx) gradCtx[k] /= batchSize;
            for (let k in gradDense) gradDense[k] /= batchSize;

            applyGradients(gradCNN, gradCtx, gradDense);
        }

        function applyGradients(gradCNN, gradCtx, gradDense) {
            const lr = CONFIG.learningRate;
            const alpha = CONFIG.momentum;

            // Apply to CNN Kernel
            for (let i = 0; i < 13; i++) {
                MODEL_MOMENTUM.cnnKernel[i] = alpha * MODEL_MOMENTUM.cnnKernel[i] + (1 - alpha) * gradCNN[i];
                AI_MODEL.cnnKernel[i] += lr * MODEL_MOMENTUM.cnnKernel[i];
                AI_MODEL.cnnKernel[i] = Math.max(0, Math.min(10, AI_MODEL.cnnKernel[i]));
            }
            // Apply to Context Weights
            for (let k in AI_MODEL.contextWeights) {
                MODEL_MOMENTUM.contextWeights[k] = alpha * MODEL_MOMENTUM.contextWeights[k] + (1 - alpha) * gradCtx[k];
                AI_MODEL.contextWeights[k] += lr * MODEL_MOMENTUM.contextWeights[k];
                AI_MODEL.contextWeights[k] = Math.max(0, Math.min(10, AI_MODEL.contextWeights[k]));
            }
            // Apply to Dense Weights
            for (let k in AI_MODEL.denseWeights) {
                MODEL_MOMENTUM.denseWeights[k] = alpha * MODEL_MOMENTUM.denseWeights[k] + (1 - alpha) * gradDense[k];
                AI_MODEL.denseWeights[k] += lr * MODEL_MOMENTUM.denseWeights[k];
                AI_MODEL.denseWeights[k] = Math.max(0, Math.min(10, AI_MODEL.denseWeights[k]));
            }
            updateNeuralUI();
        }

        // --- AUDIO ENGINE ---

        function initAudioContext() {
            if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            if (audioCtx.state === 'suspended') audioCtx.resume();
        }

        async function handleFiles(files) {
            initAudioContext();
            document.getElementById('overlay').style.display = 'none';
            const colors = ['#00ffcc', '#ff0055', '#ffff00', '#0099ff', '#ff9900', '#cc00ff'];
            if (typeof Meyda === 'undefined') {
                alert("Error: Meyda library failed to load.");
                return;
            }

            loadedTracks = [];
            sampleLibrary = [];
            transitionGraph = [];
            trailHistory = [];
            previewPath = [];
            targetSampleIndex = -1;
            contextHistory = [];
            currentSampleIndex = -1;
            prevSampleIndex = -1;

            for (let i = 0; i < files.length; i++) {
                const file = files[i];
                updateStatus(`Decoding: ${file.name}...`);
                try {
                    const arrayBuffer = await file.arrayBuffer();
                    const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
                    const detectedBpm = detectBPM(audioBuffer);
                    loadedTracks.push({
                        buffer: audioBuffer,
                        name: file.name,
                        color: colors[loadedTracks.length % colors.length],
                        bpm: detectedBpm || CONFIG.defaultBpm,
                        totalDuration: audioBuffer.duration
                    });
                } catch (e) { console.error(e); }
            }
            if (loadedTracks.length > 0) resegmentAll();
        }

        function detectBPM(buffer) {
            // Very Basic Peak Detection for demo purposes
            try {
                const data = buffer.getChannelData(0);
                const sr = buffer.sampleRate;
                const max = Math.min(data.length, sr * 30);
                let peaks = [];
                for (let i = 0; i < max; i += 4000) {
                    let m = 0;
                    for (let j = 0; j < 4000; j++) if (Math.abs(data[i + j]) > m) m = Math.abs(data[i + j]);
                    if (m > 0.3) peaks.push(i);
                }
                if (peaks.length < 5) return 120;
                let intervals = {};
                for (let i = 1; i < peaks.length; i++) {
                    let inv = peaks[i] - peaks[i - 1];
                    if (inv > sr * 0.3 && inv < sr * 1.5) {
                        let q = Math.round(inv / 1000) * 1000;
                        intervals[q] = (intervals[q] || 0) + 1;
                    }
                }
                let best = sr; let maxC = 0;
                for (let k in intervals) if (intervals[k] > maxC) { maxC = intervals[k]; best = parseInt(k); }
                let bpm = 60 / (best / sr);
                while (bpm < 75) bpm *= 2;
                while (bpm > 160) bpm /= 2;
                return Math.round(bpm);
            } catch (e) { return 120; }
        }

        function resegmentAll() {
            sampleLibrary = [];
            currentSampleIndex = -1;
            stopDJ();
            playQueue = [];
            transitionGraph = [];
            contextHistory = [];
            previewPath = [];
            targetSampleIndex = -1;
            prevSampleIndex = -1;

            updateStatus(`Neural Analysis (${CONFIG.beatsPerSegment} beats)...`);
            setTimeout(() => {
                loadedTracks.forEach((track, idx) => {
                    processAudioBuffer(track.buffer, idx, track.bpm, track.totalDuration);
                });
                if (sampleLibrary.length > 0) {
                    calculateSpace();
                    buildTransitionGraph();
                    drawCanvas();
                    document.getElementById('btn-play').disabled = false;
                    document.getElementById('btn-stop').disabled = true;
                    updateStatus("Ready. Click nodes to preview paths.");
                } else {
                    updateStatus("No usable segments found.");
                }
            }, 100);
        }

        function processAudioBuffer(buffer, trackIndex, bpm, totalDuration) {
            const rawData = buffer.getChannelData(0);
            const duration = (60 / bpm) * CONFIG.beatsPerSegment;
            const sps = Math.floor(duration * buffer.sampleRate);

            for (let i = 0; i < rawData.length; i += sps) {
                if (sampleLibrary.length >= CONFIG.maxSamples) break;
                if (i + sps > rawData.length) break;

                const segmentRaw = rawData.slice(i, i + sps);
                const f = extractFeatures(segmentRaw, CONFIG.bufferSize);
                if (f.rms < CONFIG.silenceThreshold) continue;

                sampleLibrary.push({
                    id: 0, // Set later
                    buffer: buffer,
                    startFn: i / buffer.sampleRate,
                    duration: duration,
                    bpm: bpm,
                    baseRate: 1.0,
                    features: f,
                    trackIndex: trackIndex,
                    relPos: (i / buffer.sampleRate) / totalDuration,
                    key: f.keyIndex,
                    type: 'body'
                });
            }

            const trackSamps = sampleLibrary.filter(s => s.trackIndex === trackIndex);
            if (trackSamps.length > 0) {
                // First 4 segments are "start", last 4 are "end"
                trackSamps.slice(0, 4).forEach(s => s.type = 'start');
                trackSamps.slice(-4).forEach(s => s.type = 'end');
            }

            // Re-index
            sampleLibrary.forEach((s, i) => s.id = i);
        }

        function extractFeatures(signal, bs) {
            // Simplified Feature Extraction
            let tRms = 0, tMfcc = new Array(13).fill(0), tChroma = new Array(12).fill(0), tFlat = 0, frames = 0;
            const hops = 4; const step = Math.floor((signal.length - bs) / hops);

            for (let j = 0; j < hops; j++) {
                const chunk = signal.slice(j * step, j * step + bs);
                try {
                    const f = Meyda.extract(['rms', 'mfcc', 'chroma', 'spectralFlatness'], chunk);
                    if (f) {
                        tRms += f.rms; tFlat += f.spectralFlatness;
                        for (let k = 0; k < 13; k++) tMfcc[k] += f.mfcc[k];
                        for (let k = 0; k < 12; k++) tChroma[k] += f.chroma[k];
                        frames++;
                    }
                } catch (e) { }
            }
            if (frames === 0) frames = 1;
            const avgChroma = tChroma.map(v => v / frames);
            let maxK = 0, maxV = 0; for (let k = 0; k < 12; k++) if (avgChroma[k] > maxV) { maxV = avgChroma[k]; maxK = k; }

            return {
                rms: tRms / frames,
                mfcc: tMfcc.map(v => v / frames),
                flatness: tFlat / frames,
                keyIndex: maxK
            };
        }

        function calculateSpace() {
            if (sampleLibrary.length === 0) return;
            const xVals = sampleLibrary.map(s => s.features.mfcc[1]);
            const yVals = sampleLibrary.map(s => s.features.mfcc[2]);
            const minX = Math.min(...xVals), maxX = Math.max(...xVals);
            const minY = Math.min(...yVals), maxY = Math.max(...yVals);

            sampleLibrary.forEach(s => {
                s.x = (s.features.mfcc[1] - minX) / (maxX - minX || 1);
                s.y = (s.features.mfcc[2] - minY) / (maxY - minY || 1);
                s.x += (Math.random() - 0.5) * 0.01;
                s.y += (Math.random() - 0.5) * 0.01;
            });
        }

        function buildTransitionGraph() {
            const N = sampleLibrary.length;
            transitionGraph = new Array(N);
            if (N === 0) return;

            const maxNeighbors = CONFIG.maxNeighbors || 64;

            for (let i = 0; i < N; i++) {
                const a = sampleLibrary[i];
                const local = [];

                for (let j = 0; j < N; j++) {
                    if (i === j) continue;
                    const b = sampleLibrary[j];

                    if (a.type === 'end' && b.type === 'end') continue;

                    // cheap low-dim pre-distance
                    const dM1 = a.features.mfcc[1] - b.features.mfcc[1];
                    const dM2 = a.features.mfcc[2] - b.features.mfcc[2];
                    const timbreApprox = dM1 * dM1 + dM2 * dM2;

                    const bpmApprox = Math.abs(a.bpm - b.bpm) / 40;
                    const keyDiff = Math.abs((b.key - a.key) * 7) % 12;
                    const keyApprox = Math.min(keyDiff, 12 - keyDiff) / 6;

                    const score = timbreApprox + bpmApprox + keyApprox;
                    local.push({ id: j, score });
                }

                local.sort((p, q) => p.score - q.score);
                transitionGraph[i] = local.slice(0, maxNeighbors).map(e => e.id);
            }
        }

        // --- DJ LOGIC ---

        function startDJ() {
            if (isPlaying || sampleLibrary.length === 0) return;
            initAudioContext();
            isPlaying = true;
            document.getElementById('btn-play').disabled = true;
            document.getElementById('btn-stop').disabled = false;
            document.getElementById('fileInput').disabled = true;
            nextStartTime = audioCtx.currentTime + 0.1;

            if (currentSampleIndex === -1) {
                const starts = sampleLibrary.filter(s => s.type === 'start');
                currentSampleIndex = starts.length > 0 ? starts[Math.floor(Math.random() * starts.length)].id : 0;
            }

            playQueue = [];
            trailHistory = [];
            previewPath = [];
            targetSampleIndex = -1;

            currentBpm = sampleLibrary[currentSampleIndex].bpm;
            scheduleNextSegment();
        }

        function stopDJ() {
            isPlaying = false;
            clearTimeout(playLoopId);
            document.getElementById('btn-play').disabled = sampleLibrary.length === 0;
            document.getElementById('btn-stop').disabled = true;
            document.getElementById('fileInput').disabled = false;
            document.getElementById('voting-panel').classList.remove('visible');
            updateStatus("Stopped.");
        }

        function scheduleNextSegment() {
            if (!isPlaying) return;

            // Shift Context
            if (currentSampleIndex !== -1) {
                contextHistory.push(currentSampleIndex);
                if (contextHistory.length > 3) contextHistory.shift();
            }

            // Pick Next
            if (playQueue.length > 0) currentSampleIndex = playQueue.shift();
            else currentSampleIndex = neuralPathfinder(currentSampleIndex);

            const currentSample = sampleLibrary[currentSampleIndex];

            // Voting Trigger?
            if (prevSampleIndex !== -1) {
                addTrail(prevSampleIndex, currentSampleIndex);
                document.getElementById('voting-panel').classList.add('visible');
                lastInputVector = extractTransitionVector(prevSampleIndex, currentSampleIndex);
            }

            // Sync BPM & Key
            let rate = currentSample.baseRate;
            let displayKey = currentSample.key;

            if (prevSampleIndex !== -1) {
                const ratio = currentBpm / currentSample.bpm;
                const safeRatio = Math.max(0.8, Math.min(1.2, ratio));
                rate = currentSample.baseRate * safeRatio;
                currentBpm = (currentBpm * 0.8) + (currentSample.bpm * 0.2);
            } else {
                currentBpm = currentSample.bpm;
            }

            playSegment(currentSample, nextStartTime, rate);

            // Update UI
            document.getElementById('harmonic-info').innerHTML =
                `<strong>${NOTES[displayKey]}</strong> | ${Math.round(currentBpm)} BPM<br>` +
                `<small>${loadedTracks[currentSample.trackIndex].name}</small>`;
            updateStatus(`Playing [${currentSample.type}]`);

            // Schedule Loop
            const dur = currentSample.duration / rate;
            const overlap = CONFIG.crossfade;
            nextStartTime += (dur - overlap);
            prevSampleIndex = currentSampleIndex;

            playLoopId = setTimeout(scheduleNextSegment, (dur - overlap) * 1000);
        }

        function playSegment(s, time, rate) {
            const src = audioCtx.createBufferSource();
            src.buffer = s.buffer;
            src.playbackRate.value = rate;
            const gain = audioCtx.createGain();
            const filter = audioCtx.createBiquadFilter();
            filter.type = "lowpass";

            src.connect(filter);
            filter.connect(gain);
            gain.connect(audioCtx.destination);
            src.start(time, s.startFn, s.duration);

            const fade = CONFIG.crossfade;
            gain.gain.setValueCurveAtTime(FADE_IN_CURVE, time, fade);

            // Filter Sweep Effect on transition
            filter.frequency.setValueAtTime(400, time);
            filter.frequency.exponentialRampToValueAtTime(20000, time + fade);

            const end = time + (s.duration / rate);
            if ((s.duration / rate) > fade * 2) {
                filter.frequency.setValueAtTime(20000, end - fade);
                filter.frequency.exponentialRampToValueAtTime(400, end);
            }
            gain.gain.setValueCurveAtTime(FADE_OUT_CURVE, end - fade, fade);
        }

        // --- NEURAL PATHFINDING / FEATURES ---

        function extractTransitionVector(uIdx, vIdx, ctxOverride) {
            const u = sampleLibrary[uIdx], v = sampleLibrary[vIdx];
            if (!u || !v) return null;

            const ctxHistory = ctxOverride || contextHistory;

            // --- Timbre distance (MFCC space) ---
            const mfccDiff = u.features.mfcc.map((x, i) => Math.abs(x - v.features.mfcc[i]));
            const distCurrent = Math.sqrt(
                mfccDiff.reduce((a, b) => a + b * b, 0)
            );

            // --- Distance from "natural" next in same track ---
            let distNextNat = distCurrent;
            const natIdx = uIdx + 1;
            if (
                natIdx < sampleLibrary.length &&
                sampleLibrary[natIdx].trackIndex === u.trackIndex
            ) {
                const n = sampleLibrary[natIdx];
                const dNat = n.features.mfcc.map(
                    (x, i) => Math.abs(x - v.features.mfcc[i])
                );
                distNextNat = Math.sqrt(
                    dNat.reduce((acc, val) => acc + val * val, 0)
                );
            }

            // --- Distances wrt. previous context (two steps back) ---
            let distPrev1 = 0;
            let distPrev2 = 0;

            if (ctxHistory && ctxHistory.length > 0) {
                const p1Idx = ctxHistory[ctxHistory.length - 1];
                if (p1Idx !== uIdx && sampleLibrary[p1Idx]) {
                    const p1 = sampleLibrary[p1Idx];
                    const d1 = p1.features.mfcc.map(
                        (x, i) => Math.abs(x - v.features.mfcc[i])
                    );
                    distPrev1 = Math.sqrt(
                        d1.reduce((a, b) => a + b * b, 0)
                    );
                }
            }
            if (ctxHistory && ctxHistory.length > 1) {
                const p2Idx = ctxHistory[ctxHistory.length - 2];
                if (p2Idx !== uIdx && sampleLibrary[p2Idx]) {
                    const p2 = sampleLibrary[p2Idx];
                    const d2 = p2.features.mfcc.map(
                        (x, i) => Math.abs(x - v.features.mfcc[i])
                    );
                    distPrev2 = Math.sqrt(
                        d2.reduce((a, b) => a + b * b, 0)
                    );
                }
            }

            // --- Structural / musical features ---
            const posFlow = (v.relPos < u.relPos) ? 1.0 : 0.0; // going backwards in the track

            // circle-of-fifths distance between keys
            const circleDiff = Math.abs((v.key - u.key) * 7) % 12;
            const harmony = Math.min(circleDiff, 12 - circleDiff);

            const bpmCost = Math.abs(u.bpm - v.bpm) / 20;
            const rmsDelta = Math.abs(u.features.rms - v.features.rms);
            const flatnessDelta = Math.abs(u.features.flatness - v.features.flatness);

            // how far we jump in the track (0..1 roughly)
            const relPosJump = Math.abs(v.relPos - u.relPos);

            // penalise repeating the same track too soon
            let trackRepeat = 0;
            if (ctxHistory && ctxHistory.length > 0) {
                for (let i = ctxHistory.length - 1, step = 0; i >= 0; i--, step++) {
                    const idx = ctxHistory[i];
                    const s = sampleLibrary[idx];
                    if (!s) continue;
                    if (s.trackIndex === v.trackIndex) {
                        const norm = Math.max(1, ctxHistory.length - 1);
                        trackRepeat = 1 - (step / norm); // more recent -> higher
                        break;
                    }
                }
            }

            return {
                mfccDiff,
                distCurrent,
                distNextNat,
                distPrev1,
                distPrev2,
                posFlow,
                harmony,
                bpm: bpmCost,
                rmsDelta,
                flatnessDelta,
                relPosJump,
                trackRepeat
            };
        }

        function calculateCost(uIdx, vIdx, ctxOverride) {
            const vec = extractTransitionVector(uIdx, vIdx, ctxOverride);
            if (!vec) return Infinity;

            let cost = 0;

            // CNN over MFCC differences
            for (let i = 0; i < 13; i++) {
                cost += vec.mfccDiff[i] * AI_MODEL.cnnKernel[i];
            }

            // pairwise distances
            cost += vec.distCurrent * AI_MODEL.contextWeights.wCurrent;
            cost += vec.distNextNat * AI_MODEL.contextWeights.wNextNat;
            cost += vec.distPrev1 * AI_MODEL.contextWeights.wPrev1;
            cost += vec.distPrev2 * AI_MODEL.contextWeights.wPrev2;

            // dense features
            cost += vec.posFlow * AI_MODEL.denseWeights.posFlow;
            cost += (vec.harmony / 6) * AI_MODEL.denseWeights.harmony;
            cost += vec.bpm * AI_MODEL.denseWeights.bpm;
            cost += vec.rmsDelta * AI_MODEL.denseWeights.rmsDelta;
            cost += vec.flatnessDelta * AI_MODEL.denseWeights.flatnessDelta;
            cost += vec.relPosJump * AI_MODEL.denseWeights.relPosJump;
            cost += vec.trackRepeat * AI_MODEL.denseWeights.trackRepeat;

            return cost;
        }

        function neuralPathfinder(currIdx) {
            const N = sampleLibrary.length;
            if (N === 0 || currIdx < 0 || currIdx >= N) return 0;

            const curr = sampleLibrary[currIdx];

            // --- 0. End segments: deliberately jump to a "start" ---
            if (curr.type === 'end') {
                const starts = sampleLibrary.filter(
                    s => s.type === 'start' && s.trackIndex !== curr.trackIndex
                );
                if (starts.length === 0) {
                    const neigh = transitionGraph[currIdx] || [];
                    if (neigh.length === 0) return (currIdx + 1) % N;
                    let best = neigh[0];
                    let bestCost = calculateCost(currIdx, best);
                    for (let i = 1; i < neigh.length; i++) {
                        const c = neigh[i];
                        const cost = calculateCost(currIdx, c);
                        if (cost < bestCost) {
                            bestCost = cost;
                            best = c;
                        }
                    }
                    return best;
                }
                const scoredStarts = starts.map(s => ({
                    id: s.id,
                    cost: calculateCost(currIdx, s.id)
                }));
                scoredStarts.sort((a, b) => a.cost - b.cost);
                return scoredStarts[0].id;
            }

            // --- 1. Candidate pool from precomputed neighbours ---
            let candidateIds = (transitionGraph[currIdx] || []).slice();

            // Always consider the "natural" next segment in the same track
            const natIdx = curr.id + 1;
            const hasNat = (
                natIdx < N &&
                sampleLibrary[natIdx].trackIndex === curr.trackIndex
            );
            if (hasNat) candidateIds.push(natIdx);

            if (candidateIds.length === 0) {
                // Fallback: random pool if no neighbours (shouldn't happen often)
                for (let i = 0; i < 32; i++) {
                    const r = Math.floor(Math.random() * N);
                    if (r !== currIdx) candidateIds.push(r);
                }
                candidateIds = [...new Set(candidateIds)];
            }

            // --- 2. Score candidates ---
            const seen = new Set();
            const scored = [];
            for (let i = 0; i < candidateIds.length; i++) {
                const id = candidateIds[i];
                if (id === currIdx || seen.has(id)) continue;
                const target = sampleLibrary[id];
                if (!target) continue;

                // Don't jump from middle of a track to the *start* of another track
                if (
                    curr.type === 'body' &&
                    target.type === 'start' &&
                    target.trackIndex !== curr.trackIndex
                ) {
                    continue;
                }

                seen.add(id);
                const cost = calculateCost(currIdx, id);
                scored.push({ id, cost });
            }

            if (scored.length === 0) {
                return (currIdx + 1) % N;
            }

            scored.sort((a, b) => a.cost - b.cost);

            // --- 3. Natural flow override if creative jump is too expensive ---
            let natCost = Infinity;
            if (hasNat) {
                natCost = calculateCost(currIdx, natIdx);
                const bestCost = scored[0].cost;
                if (bestCost > CONFIG.mixThreshold && natCost <= bestCost * 1.2) {
                    return natIdx;
                }
            }

            // --- 4. Softmax over top-K for infinite variety ---
            const K = Math.min(5, scored.length);
            const beta = CONFIG.selectionTemperature || 0.8;
            const top = scored.slice(0, K);

            let total = 0;
            const weights = new Array(K);
            for (let i = 0; i < K; i++) {
                const w = Math.exp(-beta * top[i].cost);
                weights[i] = w;
                total += w;
            }

            let r = Math.random() * total;
            for (let i = 0; i < K; i++) {
                r -= weights[i];
                if (r <= 0) return top[i].id;
            }
            return top[0].id;
        }

        // --- PATH PREVIEW (SIMULATED, NO AUDIO) ---

        function neuralPathfinderSim(currIdx, simContext, targetIdx, visited) {
            const N = sampleLibrary.length;
            if (N === 0 || currIdx < 0 || currIdx >= N) return null;

            const curr = sampleLibrary[currIdx];

            let candidateIds = (transitionGraph[currIdx] || []).slice();
            const natIdx = curr.id + 1;
            const hasNat = (natIdx < N && sampleLibrary[natIdx].trackIndex === curr.trackIndex);
            if (hasNat) candidateIds.push(natIdx);

            if (candidateIds.length === 0) {
                for (let i = 0; i < 32; i++) {
                    const r = Math.floor(Math.random() * N);
                    if (r !== currIdx) candidateIds.push(r);
                }
                candidateIds = [...new Set(candidateIds)];
            }

            const seen = new Set();
            const scored = [];

            for (let i = 0; i < candidateIds.length; i++) {
                const id = candidateIds[i];
                if (id === currIdx || seen.has(id)) continue;
                if (visited && visited.has(id)) continue;
                const target = sampleLibrary[id];
                if (!target) continue;

                if (
                    curr.type === 'body' &&
                    target.type === 'start' &&
                    target.trackIndex !== curr.trackIndex
                ) {
                    continue;
                }

                seen.add(id);
                const baseCost = calculateCost(currIdx, id, simContext);

                let heuristic = 0;
                if (targetIdx != null && targetIdx >= 0 && targetIdx < N) {
                    const t = sampleLibrary[targetIdx];
                    const dx = target.features.mfcc[1] - t.features.mfcc[1];
                    const dy = target.features.mfcc[2] - t.features.mfcc[2];
                    const timbreApprox = dx * dx + dy * dy;

                    const keyDiff = Math.abs((target.key - t.key) * 7) % 12;
                    const keyApprox = Math.min(keyDiff, 12 - keyDiff) / 6;

                    heuristic = timbreApprox + keyApprox;
                }

                const totalCost = baseCost + heuristic * 0.15;
                scored.push({ id, cost: totalCost });
            }

            if (scored.length === 0) return null;
            scored.sort((a, b) => a.cost - b.cost);
            return scored[0].id;
        }

        function computePathPreview(startIdx, targetIdx, maxSteps) {
            const path = [];
            const N = sampleLibrary.length;
            if (N === 0) return path;

            if (startIdx < 0 || startIdx >= N) startIdx = 0;
            if (targetIdx < 0 || targetIdx >= N) {
                path.push(startIdx);
                return path;
            }

            let simContext = contextHistory.slice();
            let currIdx = startIdx;
            const visited = new Set();

            path.push(currIdx);
            visited.add(currIdx);

            if (currIdx === targetIdx) return path;

            for (let step = 0; step < maxSteps; step++) {
                simContext.push(currIdx);
                if (simContext.length > 3) simContext.shift();

                const nextIdx = neuralPathfinderSim(currIdx, simContext, targetIdx, visited);
                if (nextIdx == null) break;

                path.push(nextIdx);
                if (nextIdx === targetIdx) break;

                if (visited.has(nextIdx)) break;
                visited.add(nextIdx);
                currIdx = nextIdx;
            }

            return path;
        }

        // --- UI & CANVAS ---

        function initCanvas() {
            canvas = document.getElementById('djCanvas');
            ctx = canvas.getContext('2d');
            resizeCanvas();
            window.addEventListener('resize', resizeCanvas);
            canvas.addEventListener('click', handleCanvasClick);
            initBrain();
            animate();
        }

        function resizeCanvas() {
            canvas.width = document.getElementById('main-container').offsetWidth;
            canvas.height = document.getElementById('main-container').offsetHeight;
        }

        function drawCanvas() {
            // Intentionally empty: drawing is handled in animate() loop
        }

        function animate() {
            requestAnimationFrame(animate);
            ctx.fillStyle = '#000';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const w = canvas.width, h = canvas.height;

            // Draw Trails (played transitions)
            trailHistory.forEach(t => {
                ctx.beginPath();
                ctx.moveTo(t.x1 * w, t.y1 * h);
                ctx.lineTo(t.x2 * w, t.y2 * h);
                ctx.strokeStyle = `rgba(0, 255, 204, ${t.life / 180})`;
                ctx.lineWidth = 2;
                ctx.setLineDash([]);
                ctx.stroke();
                t.life--;
            });
            trailHistory = trailHistory.filter(t => t.life > 0);

            // Draw Preview Path (simulated)
            if (previewPath && previewPath.length > 1) {
                ctx.save();
                ctx.lineWidth = 3;
                ctx.setLineDash([6, 4]);
                ctx.strokeStyle = 'rgba(255, 255, 0, 0.9)';
                for (let i = 0; i < previewPath.length - 1; i++) {
                    const s1 = sampleLibrary[previewPath[i]];
                    const s2 = sampleLibrary[previewPath[i + 1]];
                    if (!s1 || !s2) continue;
                    ctx.beginPath();
                    ctx.moveTo(s1.x * w, s1.y * h);
                    ctx.lineTo(s2.x * w, s2.y * h);
                    ctx.stroke();
                }
                ctx.restore();
            }

            ctx.setLineDash([]);

            // Draw Nodes
            sampleLibrary.forEach(s => {
                const x = s.x * w, y = s.y * h;
                const isCurr = (s.id === currentSampleIndex);

                ctx.beginPath();
                if (s.type === 'start') {
                    ctx.moveTo(x, y - 5);
                    ctx.lineTo(x + 5, y + 5);
                    ctx.lineTo(x - 5, y + 5);
                } else if (s.type === 'end') {
                    ctx.rect(x - 4, y - 4, 8, 8);
                } else {
                    ctx.arc(x, y, isCurr ? 6 : 2, 0, Math.PI * 2);
                }

                ctx.fillStyle = loadedTracks[s.trackIndex]?.color || '#888';
                if (isCurr) {
                    ctx.shadowBlur = 15;
                    ctx.shadowColor = '#fff';
                    ctx.fillStyle = '#fff';
                } else {
                    ctx.shadowBlur = 0;
                }
                ctx.fill();
            });

            // Highlight target sample if any
            if (targetSampleIndex !== -1) {
                const t = sampleLibrary[targetSampleIndex];
                if (t) {
                    ctx.beginPath();
                    ctx.arc(t.x * w, t.y * h, 10, 0, Math.PI * 2);
                    ctx.strokeStyle = '#ffff00';
                    ctx.lineWidth = 2;
                    ctx.stroke();
                }
            }
        }

        function handleCanvasClick(evt) {
            if (!sampleLibrary.length) return;

            const rect = canvas.getBoundingClientRect();
            const xNorm = (evt.clientX - rect.left) / canvas.width;
            const yNorm = (evt.clientY - rect.top) / canvas.height;

            let bestId = -1;
            let bestDist = Infinity;
            sampleLibrary.forEach(s => {
                const dx = s.x - xNorm;
                const dy = s.y - yNorm;
                const d = dx * dx + dy * dy;
                if (d < bestDist) {
                    bestDist = d;
                    bestId = s.id;
                }
            });

            const threshold = 0.02 * 0.02; // about 2% of canvas
            if (bestId !== -1 && bestDist < threshold) {
                targetSampleIndex = bestId;

                let startIdx = currentSampleIndex;
                if (startIdx === -1) {
                    const firstStart = sampleLibrary.find(s => s.type === 'start');
                    startIdx = firstStart ? firstStart.id : 0;
                }

                previewPath = computePathPreview(startIdx, targetSampleIndex, 32);

                const target = sampleLibrary[targetSampleIndex];
                const trackName = loadedTracks[target.trackIndex]?.name || "Unknown";
                updateStatus(`Preview path: ${previewPath.length - 1} steps ‚Üí ${NOTES[target.key]} / ${Math.round(target.bpm)} BPM (${trackName})`);
            } else {
                targetSampleIndex = -1;
                previewPath = [];
                updateStatus("Path preview cleared.");
            }
        }

        function updateNeuralUI() {
            const viz = document.getElementById('cnn-viz');
            if (!viz) return;
            viz.innerHTML = '';
            AI_MODEL.cnnKernel.forEach(val => {
                const d = document.createElement('div');
                d.className = 'kernel-cell';
                const c = Math.floor(Math.min(1, val / 5) * 255);
                d.style.backgroundColor = `rgb(${c}, 0, ${255 - c})`;
                viz.appendChild(d);
            });
            document.getElementById('val-current').innerText = AI_MODEL.contextWeights.wCurrent.toFixed(2);
            document.getElementById('val-nextnat').innerText = AI_MODEL.contextWeights.wNextNat.toFixed(2);
            document.getElementById('val-harmony').innerText = AI_MODEL.denseWeights.harmony.toFixed(2);
            document.getElementById('val-bpm').innerText = AI_MODEL.denseWeights.bpm.toFixed(2);
        }

        function updateStatus(msg) {
            const el = document.getElementById('status');
            if (el) el.innerText = msg;
        }

        function addTrail(a, b) {
            const s1 = sampleLibrary[a], s2 = sampleLibrary[b];
            if (!s1 || !s2) return;
            trailHistory.push({ x1: s1.x, y1: s1.y, x2: s2.x, y2: s2.y, life: 180 });
        }

        function toggleDevPanel() {
            document.getElementById('dev-panel').classList.toggle('visible');
        }

        // --- BINDINGS ---
        window.onload = initCanvas;
        document.getElementById('fileInput').addEventListener('change', (e) => handleFiles(e.target.files));
        document.getElementById('btn-play').onclick = startDJ;
        document.getElementById('btn-stop').onclick = stopDJ;
        document.getElementById('btn-dev').onclick = toggleDevPanel;
        document.getElementById('beatSelect').addEventListener('change', (e) => {
            CONFIG.beatsPerSegment = parseFloat(e.target.value);
            if (loadedTracks.length > 0) resegmentAll();
        });

    </script>
</body>
</html>
